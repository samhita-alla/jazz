{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import music21\n",
    "# to search for filenames with wildcard characters\n",
    "from glob import glob\n",
    "import IPython\n",
    "# tqdm is used to predict the remaining time\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from tensorflow.python.keras import utils\n",
    "import play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = glob('datasets/*.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = songs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # convert .mid file to a stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # partition a stream into parts for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "            print(typeof(parts))\n",
    "        except:\n",
    "            pass\n",
    "        # if there're instrument parts\n",
    "        if parts:\n",
    "            # To use recursion for a stream to check if there're inner substreams available\n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "        else:\n",
    "            # A very important read-only property that returns a new Stream that has all \n",
    "            # sub-containers “flattened” within it, that is, it returns a new Stream where \n",
    "            # no elements nest within other elements.\n",
    "            notes_to_parse = midi.flat.notes\n",
    "        for element in notes_to_parse:\n",
    "            # extract pitch if the element is a note\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            # append the normal form of chord(integers) to the notes list\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    with open('notes','wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notes which we'll be getting from the previous function are strings. A NN accepts inputs\n",
    "# which are real values, hence we need to map these strings into real values\n",
    "def prep_seq(notes, n_vocab):\n",
    "    seq_length = 100\n",
    "    # Remove duplicates from the notes list\n",
    "    pitchnames = sorted(set(i for i in notes))\n",
    "    # A dict to map these values and intgers\n",
    "    notes_to_int = dict((note,n) for n, note in enumerate(pitchnames))\n",
    "    net_in = []\n",
    "    net_out = []\n",
    "    # iterate over the whole notes list by selecting 100 notes every time, and the 101st will be \n",
    "    # the sequence output\n",
    "    for i in range(0, len(notes)-seq_length,1):\n",
    "        seq_in = notes[i:i+seq_length]\n",
    "        seq_out = notes[i+seq_length]\n",
    "        net_in.append([notes_to_int[j] for j in seq_in])\n",
    "        net_out.append(notes_to_int[seq_out])\n",
    "    number_of_patterns = len(net_in)\n",
    "    \n",
    "    # reshaping the input into LSTM compatible - samples, timesteps, features\n",
    "    # Samples. One sequence is one sample. A batch is comprised of one or more samples.\n",
    "    # Time Steps. One time step is one point of observation in the sample.\n",
    "    # Features. One feature is one observation at a time step.\n",
    "    net_in = np.reshape(net_in, (number_of_patterns, seq_length, 1))\n",
    "    \n",
    "    # Input normalization\n",
    "    net_in = net_in/float(n_vocab)\n",
    "    \n",
    "    net_out = utils.to_categorical(net_out)\n",
    "    return (net_in, net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, LSTM, Dense, Dropout, Flatten\n",
    "def net_arch(net_in, n_vocab):\n",
    "    model = Sequential()\n",
    "    # 128 - dimensionality of the output space.\n",
    "    # let’s say we have an input with shape (num_seq, seq_len, num_feature). If we \n",
    "    # don’t set return_sequences=True, our output will have the shape (num_seq, num_feature), \n",
    "    # but if we do, we will obtain the output with shape (num_seq, seq_len, num_feature).\n",
    "    model.add(LSTM(128, input_shape=net_in.shape[1:], return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "def train(model, net_in, net_out, epochs):\n",
    "    filepath = \"weights.best.music3.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose=0, save_best_only = True)\n",
    "    \n",
    "    model.fit(net_in, net_out, epochs = epochs, batch_size = 32, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net():\n",
    "    epochs = 100\n",
    "    notes = get_notes()\n",
    "    n_vocab = len(set(notes))\n",
    "    net_in, net_out = prep_seq(notes, n_vocab)\n",
    "    model = net_arch(net_in, n_vocab)\n",
    "    train(model, net_in, net_out, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1398/1398 [==============================] - 21s 15ms/step - loss: 2.3744\n",
      "Epoch 2/100\n",
      "1398/1398 [==============================] - 23s 16ms/step - loss: 2.1240\n",
      "Epoch 3/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 2.0443\n",
      "Epoch 4/100\n",
      "1398/1398 [==============================] - 22s 16ms/step - loss: 1.9667\n",
      "Epoch 5/100\n",
      "1398/1398 [==============================] - 20s 15ms/step - loss: 1.9414\n",
      "Epoch 6/100\n",
      "1398/1398 [==============================] - 20s 14ms/step - loss: 1.9054\n",
      "Epoch 7/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 1.8325\n",
      "Epoch 8/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 1.7375\n",
      "Epoch 9/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 1.7007\n",
      "Epoch 10/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 1.6367\n",
      "Epoch 11/100\n",
      "1398/1398 [==============================] - 19s 14ms/step - loss: 1.5679\n",
      "Epoch 12/100\n",
      "1398/1398 [==============================] - 22s 16ms/step - loss: 1.4646\n",
      "Epoch 13/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 1.3944\n",
      "Epoch 14/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 1.3242\n",
      "Epoch 15/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 1.2587\n",
      "Epoch 16/100\n",
      "1398/1398 [==============================] - 18s 13ms/step - loss: 1.1661\n",
      "Epoch 17/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 1.1112\n",
      "Epoch 18/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.9958\n",
      "Epoch 19/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.9590\n",
      "Epoch 20/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.8993\n",
      "Epoch 21/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.7634\n",
      "Epoch 22/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.7068\n",
      "Epoch 23/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.6667\n",
      "Epoch 24/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.5890\n",
      "Epoch 25/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.5239\n",
      "Epoch 26/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.4701\n",
      "Epoch 27/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.4288\n",
      "Epoch 28/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.3572\n",
      "Epoch 29/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.3416\n",
      "Epoch 30/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.3040\n",
      "Epoch 31/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2922\n",
      "Epoch 32/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2755\n",
      "Epoch 33/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2477\n",
      "Epoch 34/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2412\n",
      "Epoch 35/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2541\n",
      "Epoch 36/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2051\n",
      "Epoch 37/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.2176\n",
      "Epoch 38/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1922\n",
      "Epoch 39/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.2198\n",
      "Epoch 40/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1885\n",
      "Epoch 41/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1710\n",
      "Epoch 42/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1672\n",
      "Epoch 43/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1640\n",
      "Epoch 44/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1483\n",
      "Epoch 45/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.1550\n",
      "Epoch 46/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1422\n",
      "Epoch 47/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.1306\n",
      "Epoch 48/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1107\n",
      "Epoch 49/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1206\n",
      "Epoch 50/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.1170\n",
      "Epoch 51/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1060\n",
      "Epoch 52/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.1176\n",
      "Epoch 53/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1353\n",
      "Epoch 54/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1340\n",
      "Epoch 55/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1115\n",
      "Epoch 56/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1011\n",
      "Epoch 57/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0956\n",
      "Epoch 58/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0999\n",
      "Epoch 59/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0869\n",
      "Epoch 60/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0865\n",
      "Epoch 61/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0959\n",
      "Epoch 62/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0907\n",
      "Epoch 63/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0969\n",
      "Epoch 64/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.1018\n",
      "Epoch 65/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0981\n",
      "Epoch 66/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0996\n",
      "Epoch 67/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0797\n",
      "Epoch 68/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0713\n",
      "Epoch 69/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0771\n",
      "Epoch 70/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0819\n",
      "Epoch 71/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0818\n",
      "Epoch 72/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0608\n",
      "Epoch 73/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0502\n",
      "Epoch 74/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0550\n",
      "Epoch 75/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0773\n",
      "Epoch 76/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0755\n",
      "Epoch 77/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0808\n",
      "Epoch 78/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0720\n",
      "Epoch 79/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0653\n",
      "Epoch 80/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0914\n",
      "Epoch 81/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0638\n",
      "Epoch 82/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0847\n",
      "Epoch 83/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0535\n",
      "Epoch 84/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0616\n",
      "Epoch 85/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0644\n",
      "Epoch 86/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0685\n",
      "Epoch 87/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0668\n",
      "Epoch 88/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0675\n",
      "Epoch 89/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0722\n",
      "Epoch 90/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0386\n",
      "Epoch 91/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0514\n",
      "Epoch 92/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0405\n",
      "Epoch 93/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0449\n",
      "Epoch 94/100\n",
      "1398/1398 [==============================] - 17s 12ms/step - loss: 0.0320\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0401\n",
      "Epoch 96/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0618\n",
      "Epoch 97/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0433\n",
      "Epoch 98/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0484\n",
      "Epoch 99/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0500\n",
      "Epoch 100/100\n",
      "1398/1398 [==============================] - 16s 12ms/step - loss: 0.0419\n"
     ]
    }
   ],
   "source": [
    "train_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputseq(notes, pitchnames, n_vocab):\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = 100\n",
    "    network_input = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "\n",
    "    return (network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, net_in, pitchnames, n_vocab):\n",
    "    start = np.random.randint(0, len(net_in)-1)\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    pattern = net_in[start]\n",
    "    prediction_output = []\n",
    "    print('Generating notes')\n",
    "    # Generating 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern,(1,len(pattern),1))\n",
    "        prediction_input = prediction_input/float(n_vocab)\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "        # add the generated index of the character and proceed by not considering the first char\n",
    "        # in each iteration\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    print('Notes generated')\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    # create notes and chord objects\n",
    "    for pattern in prediction_output:\n",
    "        # Chord\n",
    "        if('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # Note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        offset += 0.5 # increase offset so that notes do not get clumsy\n",
    "    \n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    print('MIDI file save')\n",
    "    midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    # loading notes\n",
    "    with open('notes','rb') as filepath:\n",
    "        notes = pickle.load(filepath)\n",
    "    pitchnames = sorted(set(i for i in notes))\n",
    "    n_vocab = len(pitchnames)\n",
    "    print('Start music generation.')\n",
    "    net_in = get_inputseq(notes, pitchnames, n_vocab)\n",
    "    # print(type(net_in[0]))\n",
    "    normalized_in = np.reshape(net_in, (len(net_in), 100, 1))\n",
    "    # Input normalization\n",
    "    normalized_in = normalized_in/float(n_vocab)\n",
    "    model = net_arch(normalized_in, n_vocab)\n",
    "    \n",
    "    model.load_weights('weights.best.music3.hdf5')\n",
    "    prediction_output = generate_notes(model, net_in, pitchnames, n_vocab)\n",
    "    create_midi(prediction_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start music generation.\n",
      "Generating notes\n",
      "Notes generated\n",
      "MIDI file save\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music file test_output.mid loaded!\n"
     ]
    }
   ],
   "source": [
    "play.play_midi('test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
